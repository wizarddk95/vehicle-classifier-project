import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, log_loss
from tqdm import tqdm

from timm.data import Mixup
from timm.loss import SoftTargetCrossEntropy


class EarlyStopping:
    def __init__(self, patience=3, verbose=True):
        self.patience = patience
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False
        self.verbose = verbose
        self.best_model_state = None

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.best_model_state = model.state_dict()
            self.counter = 0
        else:
            self.counter += 1
            if self.verbose:
                print(f"ðŸ“‰ EarlyStopping: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True

def multiclass_log_loss(answer_df, submission_df):
    # í´ëž˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìžì—´ë¡œ ì •ë¦¬
    class_list = sorted([str(label) for label in answer_df['label'].unique()])

    # ID ê¸°ì¤€ ì •ë ¬
    submission_df = submission_df.sort_values(by='ID').reset_index(drop=True)
    answer_df = answer_df.sort_values(by='ID').reset_index(drop=True)

    # ID ì¼ì¹˜ ê²€ì¦
    if not all(answer_df['ID'] == submission_df['ID']):
        raise ValueError("IDê°€ ì •ë ¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤.")

    # labelë„ ë¬¸ìžì—´ë¡œ ë³€í™˜
    answer_df['label'] = answer_df['label'].astype(str)

    # ì œì¶œ ì»¬ëŸ¼ ê²€ì¦
    missing_cols = [col for col in class_list if col not in submission_df.columns]
    if missing_cols:
        raise ValueError(f"í´ëž˜ìŠ¤ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")

    if submission_df[class_list].isnull().any().any():
        raise ValueError("NaN í¬í•¨ë¨")

    for col in class_list:
        if not ((submission_df[col] >= 0) & (submission_df[col] <= 1)).all():
            raise ValueError(f"{col}ì˜ í™•ë¥ ê°’ì´ 0~1 ë²”ìœ„ ì´ˆê³¼")

    # ì •ë‹µ ì¸ë±ìŠ¤ ë³€í™˜
    true_labels = answer_df['label'].tolist()
    true_idx = [class_list.index(lbl) for lbl in true_labels]

    # í™•ë¥  ì •ê·œí™” + clip
    probs = submission_df[class_list].values
    probs = probs / probs.sum(axis=1, keepdims=True)
    y_pred = np.clip(probs, 1e-15, 1 - 1e-15)

    return log_loss(true_idx, y_pred, labels=list(range(len(class_list))))

def train_one_epoch(model, dataloader, criterion, optimizer, device, augment_fn=None):
    model.train()
    total_loss = 0
    for x, y, *_ in tqdm(dataloader, desc="Train", leave=False):
        x, y = x.to(device), y.to(device)

        if augment_fn is not None:
            x, y = augment_fn(x, y) # yëŠ” soft-label(one-hot)

        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x.size(0)
        
    return total_loss / len(dataloader.dataset)

def evaluate(model, dataloader, device):
    model.eval()
    probs, preds, labels, ids, img_paths = [], [], [], [], []

    with torch.no_grad():
        for i, (x, y, paths) in enumerate(tqdm(dataloader, desc="Eval", leave=False)):
            x = x.to(device)
            out = model(x)
            prob = torch.softmax(out, dim=1).cpu().numpy()

            preds += list(np.argmax(prob, axis=1))
            probs.append(prob)
            labels += y.numpy().tolist()
            ids += list(range(i * dataloader.batch_size, i * dataloader.batch_size + len(x)))
            img_paths += paths

    return preds, np.vstack(probs), labels, ids, img_paths


