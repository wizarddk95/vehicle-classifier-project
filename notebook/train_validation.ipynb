{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계층적 분할 확인\n",
      "학습 데이터 개수: 25112\n",
      "검증 데이터 개수: 6279\n",
      "총 데이터 개수: 31391\n",
      "\n",
      "🚀 학습 시작: vit_base_patch16_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 1 train_loss: 6.0251 | learning_rate: 0.000100 | ✅ Acc: 0.0037 | LogLoss: 5.9683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 2 train_loss: 5.9736 | learning_rate: 0.000100 | ✅ Acc: 0.0097 | LogLoss: 5.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 3 train_loss: 5.8780 | learning_rate: 0.000099 | ✅ Acc: 0.0137 | LogLoss: 5.6525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 4 train_loss: 5.7304 | learning_rate: 0.000099 | ✅ Acc: 0.0374 | LogLoss: 5.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 5 train_loss: 5.5528 | learning_rate: 0.000098 | ✅ Acc: 0.0771 | LogLoss: 4.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch 6 train_loss: 5.3310 | learning_rate: 0.000096 | ✅ Acc: 0.1473 | LogLoss: 4.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    133\u001b[39m early_stopping = EarlyStopping(patience=patience)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# print(f\"\\n📘 Epoch {epoch + 1}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutmix_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# train_loss = train_one_epoch(model, loaders['train'], criterion, optimizer, device)\u001b[39;00m\n\u001b[32m    139\u001b[39m     y_pred, y_prob, y_true, y_id, _ = evaluate(model, loaders[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m], device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data Analysis\\vehicle_classifier_project\\models\\train_utils.py:103\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device, mixup_fn)\u001b[39m\n\u001b[32m    101\u001b[39m     loss.backward()\n\u001b[32m    102\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * x.size(\u001b[32m0\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timm import create_model\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from dataloaders.loaders import get_dataloaders\n",
    "from models.train_utils import train_one_epoch, evaluate, EarlyStopping, multiclass_log_loss\n",
    "from analysis.result_plotter import analyze_model_output\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_cutmix(x, y, num_classes, beta=1.0):\n",
    "    lam = np.random.beta(beta, beta)\n",
    "    rand_index = torch.randperm(x.size(0)).to(x.device)\n",
    "    y1 = F.one_hot(y, num_classes=num_classes).float()\n",
    "    y2 = y1[rand_index]\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[rand_index, :, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    y_mix = y1 * lam + y2 * (1. - lam)\n",
    "    return x, y_mix\n",
    "\n",
    "# 시드 고정\n",
    "def seed_everything(seed=28):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(28)\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ✅ 설정\n",
    "data_root = '../data/train2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 40\n",
    "patience = 4\n",
    "batch_size = 32\n",
    "base_log_dir = \"../runs\"\n",
    "os.makedirs(base_log_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 데이터 로더 구성\n",
    "dataloaders = get_dataloaders(data_root, batch_size=batch_size)\n",
    "\n",
    "# 첫 번째 모델의 데이터 수 확인\n",
    "first_model_name = list(dataloaders.keys())[0]\n",
    "train_loader = dataloaders[first_model_name]['train']\n",
    "val_loader = dataloaders[first_model_name]['val']\n",
    "print(f\"학습 데이터 개수: {len(train_loader.dataset)}\")\n",
    "print(f\"검증 데이터 개수: {len(val_loader.dataset)}\")\n",
    "print(f\"총 데이터 개수: {len(train_loader.dataset) + len(val_loader.dataset)}\")\n",
    "\n",
    "results = []\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for model_name, loaders in dataloaders.items():\n",
    "    print(f\"\\n🚀 학습 시작: {model_name}\")\n",
    "    \n",
    "    log_dir = os.path.join(base_log_dir, f\"{model_name}_{timestamp}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    model = create_model(model_name, pretrained=True, num_classes=len(loaders['classes'])).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # mixup_fn = Mixup(\n",
    "    #     mixup_alpha=0.8,\n",
    "    #     cutmix_alpha=1.0,\n",
    "    #     cutmix_minmax=None,\n",
    "    #     prob=0.5,\n",
    "    #     switch_prob=0.5,\n",
    "    #     mode='batch',\n",
    "    #     label_smoothing=0.1,\n",
    "    #     num_classes=396  # 클래스 수\n",
    "    # )\n",
    "\n",
    "        # num_classes 정의\n",
    "    num_classes = len(loaders['classes'])\n",
    "\n",
    "    # cutmix_fn 정의\n",
    "    cutmix_fn = lambda x, y: apply_cutmix(x, y, num_classes=num_classes, beta=1.0)\n",
    "\n",
    "    # criterion 변경\n",
    "    criterion = SoftTargetCrossEntropy()  # Mixup & CutMix 사용 시\n",
    "    # criterion = nn.CrossEntropyLoss()   # 사용 안 할 경우\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"\\n📘 Epoch {epoch + 1}\")\n",
    "        train_loss = train_one_epoch(model, loaders['train'], criterion, optimizer, device, cutmix_fn)\n",
    "        # train_loss = train_one_epoch(model, loaders['train'], criterion, optimizer, device)\n",
    "        y_pred, y_prob, y_true, y_id, _ = evaluate(model, loaders['val'], device)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        class_list = [str(i) for i in range(y_prob.shape[1])]\n",
    "        prob_df = pd.DataFrame(y_prob, columns=class_list)\n",
    "        prob_df.insert(0, 'ID', y_id)\n",
    "        label_df = pd.DataFrame({'ID': y_id, 'label': [str(l) for l in y_true]})\n",
    "\n",
    "        logloss = multiclass_log_loss(label_df, prob_df)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", logloss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Validation\", acc, epoch)\n",
    "        writer.add_scalar(\"LearningRate\", scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "        print(f\"📘 Epoch {epoch + 1} train_loss: {train_loss:.4f} | learning_rate: {scheduler.get_last_lr()[0]:.6f} | ✅ Acc: {acc:.4f} | LogLoss: {logloss:.4f}\")\n",
    "        scheduler.step()\n",
    "        early_stopping(logloss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break \n",
    "\n",
    "    model.load_state_dict(early_stopping.best_model_state)\n",
    "    writer.close()\n",
    "\n",
    "    # ✅ 분석 결과 자동 저장\n",
    "    val_indices = loaders['val'].dataset.indices if hasattr(loaders['val'].dataset, 'indices') else list(range(len(loaders['val'].dataset)))\n",
    "    base_dataset = loaders['val'].dataset.dataset\n",
    "    image_paths = [base_dataset.samples[i][0] for i in val_indices]\n",
    "\n",
    "    analyze_model_output(\n",
    "        model_name=model_name,\n",
    "        timestamp=timestamp,\n",
    "        image_paths=image_paths,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob,\n",
    "        y_true=y_true,\n",
    "        class_names=loaders['classes']\n",
    "    )\n",
    "\n",
    "    results.append({'model': model_name, 'accuracy': acc, 'log_loss': logloss, 'timestamp': timestamp})\n",
    "\n",
    "# ✅ 최종 비교 결과\n",
    "df_result = pd.DataFrame(results).sort_values(by='log_loss')\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
